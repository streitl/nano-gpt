{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-10T11:46:21.767901781Z",
     "start_time": "2024-01-10T11:46:21.001144565Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import BigramLanguageModel, GPTLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.manual_seed(2024);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T11:46:21.770075875Z",
     "start_time": "2024-01-10T11:46:21.767237528Z"
    }
   },
   "id": "31f22105c07b9f4a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text: str = Path(\"tiny_shakespeare.txt\").read_text()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "VOCABULARY: tuple[str, ...] = tuple(sorted(set(text)))\n",
    "VOCABULARY_SIZE: int = len(VOCABULARY)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21167f8dc6a72171"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "char2idx = {char: idx for idx, char in enumerate(VOCABULARY)}\n",
    "idx2char = {idx: char for idx, char in enumerate(VOCABULARY)}\n",
    "\n",
    "\n",
    "def encode(string: str) -> tuple[int, ...]:\n",
    "    return tuple(char2idx[c] for c in string)\n",
    "\n",
    "\n",
    "def decode(tup: tuple[int, ...]) -> str:\n",
    "    return \"\".join([idx2char[i] for i in tup])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T11:46:21.813460738Z",
     "start_time": "2024-01-10T11:46:21.771524515Z"
    }
   },
   "id": "94d9cf0e88333224",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "VALIDATION_PROPORTION: float = 0.1\n",
    "\n",
    "data: torch.Tensor = torch.tensor(encode(text), dtype=torch.long)\n",
    "n_train_samples = int((1 - VALIDATION_PROPORTION) * len(data))\n",
    "train_data = data[:n_train_samples]\n",
    "val_data = data[n_train_samples:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T11:46:21.863735082Z",
     "start_time": "2024-01-10T11:46:21.812152967Z"
    }
   },
   "id": "3c5c46656c4740a8",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_batch(split: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i : i + BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + BLOCK_SIZE + 1] for i in ix])\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T11:46:21.984246841Z",
     "start_time": "2024-01-10T11:46:21.943448498Z"
    }
   },
   "id": "28cc31f4c80baac7",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 8\n",
    "MAX_ITERS = 3000\n",
    "EVAL_INTERVAL = 300\n",
    "EVAL_ITERS = 200\n",
    "\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "model = BigramLanguageModel(vocabulary_size=VOCABULARY_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T11:46:22.398639277Z",
     "start_time": "2024-01-10T11:46:22.395615098Z"
    }
   },
   "id": "8c389138f8380774",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BLOCK_SIZE = 128\n",
    "MAX_ITERS = 5000\n",
    "EVAL_INTERVAL = 500\n",
    "EVAL_ITERS = 10\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    n_embeddings=256,\n",
    "    n_heads=3,\n",
    "    n_layers=3,\n",
    "    dropout=0.2,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    vocabulary_size=VOCABULARY_SIZE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T14:36:38.747416141Z",
     "start_time": "2024-01-10T14:36:38.704268133Z"
    }
   },
   "id": "a5232c744b2c613b",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.430529 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T14:36:14.674807704Z",
     "start_time": "2024-01-10T14:36:14.672246958Z"
    }
   },
   "id": "fd8400445426e528",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in tqdm(range(EVAL_ITERS), position=1, leave=False, desc=f\"Evaluating on {split} set\"):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T14:27:22.613287350Z",
     "start_time": "2024-01-10T14:27:22.601480927Z"
    }
   },
   "id": "f01c54241e252625",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = torch.load(\"gpt.pt\")\n",
    "# model.load(\"bigram.pt\")\n",
    "model.eval();"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T11:45:56.677347639Z",
     "start_time": "2024-01-10T11:45:56.672399332Z"
    }
   },
   "id": "40e8b5a7dc23f5a9",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 499/5000 [06:06<1:15:21,  1.00s/it]\n",
      "Evaluating on train set:   0%|          | 0/10 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating on train set:  10%|█         | 1/10 [00:00<00:02,  4.41it/s]\u001B[A\n",
      "Evaluating on train set:  20%|██        | 2/10 [00:00<00:01,  4.32it/s]\u001B[A\n",
      "Evaluating on train set:  30%|███       | 3/10 [00:00<00:01,  4.28it/s]\u001B[A\n",
      "Evaluating on train set:  40%|████      | 4/10 [00:00<00:01,  4.26it/s]\u001B[A\n",
      "Evaluating on train set:  50%|█████     | 5/10 [00:01<00:01,  4.28it/s]\u001B[A\n",
      "Evaluating on train set:  60%|██████    | 6/10 [00:01<00:00,  4.28it/s]\u001B[A\n",
      "Evaluating on train set:  70%|███████   | 7/10 [00:01<00:00,  4.31it/s]\u001B[A\n",
      "Evaluating on train set:  80%|████████  | 8/10 [00:01<00:00,  4.29it/s]\u001B[A\n",
      "Evaluating on train set:  90%|█████████ | 9/10 [00:02<00:00,  4.29it/s]\u001B[A\n",
      "Evaluating on train set: 100%|██████████| 10/10 [00:02<00:00,  4.25it/s]\u001B[A\n",
      "                                                                        \u001B[A\n",
      "Evaluating on val set:   0%|          | 0/10 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating on val set:  10%|█         | 1/10 [00:00<00:02,  4.21it/s]\u001B[A\n",
      "Evaluating on val set:  20%|██        | 2/10 [00:00<00:01,  4.07it/s]\u001B[A\n",
      "Evaluating on val set:  30%|███       | 3/10 [00:00<00:01,  3.96it/s]\u001B[A\n",
      "Evaluating on val set:  40%|████      | 4/10 [00:01<00:01,  3.91it/s]\u001B[A\n",
      "Evaluating on val set:  50%|█████     | 5/10 [00:01<00:01,  3.90it/s]\u001B[A\n",
      "Evaluating on val set:  60%|██████    | 6/10 [00:01<00:00,  4.03it/s]\u001B[A\n",
      "Evaluating on val set:  70%|███████   | 7/10 [00:01<00:00,  4.15it/s]\u001B[A\n",
      "Evaluating on val set:  80%|████████  | 8/10 [00:01<00:00,  4.13it/s]\u001B[A\n",
      "Evaluating on val set:  90%|█████████ | 9/10 [00:02<00:00,  3.69it/s]\u001B[A\n",
      "Evaluating on val set: 100%|██████████| 10/10 [00:02<00:00,  2.57it/s]\u001B[A\n",
      "                                                                      \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: train 1.7878, val 1.9337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 677/5000 [08:24<52:22,  1.38it/s]  "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for it in tqdm(range(MAX_ITERS)):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (it + 1) % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"loss: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    _, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-10T14:36:40.697489079Z"
    }
   },
   "id": "df678b1cb6c0ce85",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model, \"gpt-demo-large.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1da6739542810bae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T14:25:17.455737206Z",
     "start_time": "2024-01-10T14:25:17.414786728Z"
    }
   },
   "id": "373cb853448edb8e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Nurse:\n",
      "I am, where in to is besevery, and unto it the den,\n",
      "To And what prossided when castes trick mother,\n",
      "The gainst no earry!\n",
      "\n",
      "METRLANUS:\n",
      "No thy al pine.\n",
      "\n",
      "Second Cury!\n",
      "BRUTUS:\n",
      "my dure! True! whereford't wenty hould find\n",
      "Witht in should; do herefore inly, if\n",
      "That thart won welps the folliht them all king the nobbe wing'd,\n",
      "Ander he to drawns helence? Heavy that wars her shouldincis,\n",
      "Buth they alonge hath yet swear Duke marriator:\n",
      "And which !\n",
      "\n",
      "DUCHARD II:\n",
      "No mindes unothing ha now?\n",
      "Take unram of mall our userit on the page.\n",
      "Senator your, agood nevil, God alkay?\n",
      "\n",
      "This then as of the tet their on of words: be Juldenety.\n",
      "\n",
      "Fivost:\n",
      "Thou he a criuries honour the his commpany mind;\n",
      "And care gentlement shall it liss execes of appreemianted!\n",
      "Do, on it gaters, crafe, strtial fend\n",
      "Of it on hild country apsed;\n",
      "Breas, and thyal renoble?\n",
      "\n",
      "ESCAPULIET:\n",
      "I force no thusbatstire a nothing wall\n",
      "Indo a Rome, thart the shalt mose! God mock!\n",
      "That shall me on ion; a of to my commber\n",
      "And natulare to belierch.\n",
      "\n",
      "EDWARD:\n",
      "Ondone spease, inot soyaltagin him instell was 't\n",
      "Tak thy lo sworseby nowin what song;\n",
      "Priseddise so; follow wethe cheed, and eat with glas!\n",
      "\n",
      "PRINCE EDWARD:\n",
      "O, that not Marciady.\n",
      "\n",
      "KING VI:\n",
      "No!\n",
      "\n",
      "KING EDWICK:\n",
      "The fries motan:\n",
      "And a depewnthis law?\n",
      "\n",
      "KATHENre prced Warwick,\n",
      "They fautht but be ke the here;\n",
      "Thenefaintan balts scold beats him mercitate.\n",
      "\n",
      "ROMEO:\n",
      "Sir An, if uncly, slip, thyselent to any,\n",
      "The leas sto the doy te sound earts not abbefore. But I could award,\n",
      "Mis no to theserving hi\n"
     ]
    }
   ],
   "source": [
    "print(decode(tuple(model.generate(context, max_new_tokens=1500)[0].tolist())))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T14:25:49.530080855Z",
     "start_time": "2024-01-10T14:25:41.972584497Z"
    }
   },
   "id": "362a781224887b4b",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "16cf2243cbcb7956"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ca0ff9beb1b747e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
